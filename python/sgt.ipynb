{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Authors: Chitta Ranjan <cran2367@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inferno/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import warnings\n",
    "\n",
    "########\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "np.random.seed(7) # fix random seed for reproducibility\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import sklearn.metrics\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sgt import Sgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgt = Sgt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence = np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"])\n",
    "alphabets = [\"A\", \"B\", \"C\"]\n",
    "lengthsensitive = True\n",
    "kappa = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (array([2, 4, 6, 7, 9]),)),\n",
       " ('B', (array([0, 1, 8]),)),\n",
       " ('C', (array([3, 5]),))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgt.getpositions(sequence = sequence, alphabets = alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.369361</td>\n",
       "      <td>0.442463</td>\n",
       "      <td>0.537637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.414884</td>\n",
       "      <td>0.468038</td>\n",
       "      <td>0.162774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.454136</td>\n",
       "      <td>0.068693</td>\n",
       "      <td>0.214492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "A  0.369361  0.442463  0.537637\n",
       "B  0.414884  0.468038  0.162774\n",
       "C  0.454136  0.068693  0.214492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgt.fit(sequence, alphabets, lengthsensitive, kappa, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [[\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"], [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90616284 1.31002279 2.6184865  0.         0.         0.86569371\n",
      "  1.23042262 0.52543984 0.         0.         1.37141609 0.28262508\n",
      "  1.35335283 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09157819 0.92166965 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.92166965\n",
      "  1.45182361]]\n"
     ]
    }
   ],
   "source": [
    "s = sgt.fit_transform(corpus)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_test = [['a', 'b'], ['a', 'b', 'c'], ['e', 'f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_model_test = Sgt(kappa=10, lengthsensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = sequence_model_test.fit_transform(corpus=sequence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.39428342, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.41059877, 0.15105085, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.41059877, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.39428342,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'e', 'f']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_model_test.alphabets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein Sequence Data Analysis\n",
    "\n",
    "The data used here is taken from www.uniprot.org. This is a public database for proteins. The data contains the protein sequences and their functions. In the following, we will demonstrate \n",
    "- clustering of the sequences.\n",
    "- classification of the sequences with the functions as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'E', 'I', 'E', 'K', 'T', 'N', 'R', 'M', 'N', 'A', 'L', 'F', 'E', 'F', 'Y', 'A', 'A', 'L', 'L', 'T', 'D', 'K', 'Q', 'M', 'N', 'Y', 'I', 'E', 'L', 'Y', 'Y', 'A', 'D', 'D', 'Y', 'S', 'L', 'A', 'E', 'I', 'A', 'E', 'E', 'F', 'G', 'V', 'S', 'R', 'Q', 'A', 'V', 'Y', 'D', 'N', 'I', 'K', 'R', 'T', 'E', 'K', 'I', 'L', 'E', 'D', 'Y', 'E', 'M', 'K', 'L', 'H', 'M', 'Y', 'S', 'D', 'Y', 'I', 'V', 'R', 'S', 'Q', 'I', 'F', 'D', 'Q', 'I', 'L', 'E', 'R', 'Y', 'P', 'K', 'D', 'D', 'F', 'L', 'Q', 'E', 'Q', 'I', 'E', 'I', 'L', 'T', 'S', 'I', 'D', 'N', 'R', 'E']\n"
     ]
    }
   ],
   "source": [
    "protein_data=pd.DataFrame.from_csv('../data/protein_classification.csv')\n",
    "X=protein_data['Sequence']\n",
    "def split(word): \n",
    "    return [char for char in word] \n",
    "\n",
    "sequences = [split(x) for x in X]\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgt = Sgt(kappa = 1, lengthsensitive = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = sgt.fit_transform(corpus=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2112, 400)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Clustering\n",
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6432744907364925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384913</td>\n",
       "      <td>-0.269873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.135995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177792</td>\n",
       "      <td>-0.172454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168074</td>\n",
       "      <td>-0.147334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.383616</td>\n",
       "      <td>-0.271163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2\n",
       "0  0.384913 -0.269873\n",
       "1  0.022764  0.135995\n",
       "2  0.177792 -0.172454\n",
       "3  0.168074 -0.147334\n",
       "4  0.383616 -0.271163"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(embedding)\n",
    "X=pca.transform(embedding)\n",
    "\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "df = pd.DataFrame(data=X, columns=['x1', 'x2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13e9f7240>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEyCAYAAACYrUmUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d7b1ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, max_iter =300)\n",
    "kmeans.fit(df)\n",
    "\n",
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "colmap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "colors = list(map(lambda x: colmap[x+1], labels))\n",
    "plt.scatter(df['x1'], df['x2'], color=colors, alpha=0.5, edgecolor=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sequence Classification\n",
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = protein_data['Function [CC]']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a 10-fold cross-validation to measure the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 1.0\n"
     ]
    }
   ],
   "source": [
    "kfold = 10\n",
    "X = pd.DataFrame(embedding)\n",
    "y = encoded_y\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "skf = KFold(n_splits = kfold, shuffle = True, random_state = random_state)\n",
    "k = 0\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = X_train.as_matrix(columns = None)\n",
    "    X_test = X_test.as_matrix(columns = None)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape = (X_train.shape[1],))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1\n",
    "    \n",
    "print ('Average f1 score', np.mean(test_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weblog Data Analysis\n",
    "This data sample is taken from https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset. \n",
    "This is a network intrusion data containing audit logs and any attack as a positive label. Since, network intrusion is a rare event, the data is unbalanced. Here we will,\n",
    "- build a sequence classification model to predict a network intrusion.\n",
    "\n",
    "Each sequence contains in the data is a series of activity, for example, {login, password}. The _alphabets_ in the input data sequences are already encoded into integers. The original sequences data file is also present in the `/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['seqlen', 'seq', 'class'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darpa_data = pd.DataFrame.from_csv('../data/darpa_data.csv')\n",
    "darpa_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = darpa_data['seq']\n",
    "sequences = [x.split('~') for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = darpa_data['class']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence embeddings\n",
    "In this data, the sequence embeddings should be length-sensitive. The lengths are important here because sequences with similar patterns but different lengths can have different labels. Consider a simple example of two sessions: `{login, pswd, login, pswd,...}` and `{login, pswd,...(repeated several times)..., login, pswd}`. While the first session can be a regular user mistyping the password once, the other session is possibly an attack to guess the password. Thus, the sequence lengths are as important as the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgt_darpa = Sgt(kappa = 5, lengthsensitive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = sgt_darpa.fit_transform(corpus=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2391</th>\n",
       "      <th>2392</th>\n",
       "      <th>2393</th>\n",
       "      <th>2394</th>\n",
       "      <th>2395</th>\n",
       "      <th>2396</th>\n",
       "      <th>2397</th>\n",
       "      <th>2398</th>\n",
       "      <th>2399</th>\n",
       "      <th>2400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.804190e-09</td>\n",
       "      <td>7.041516e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004958e-12</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>1.046458e-07</td>\n",
       "      <td>5.863092e-16</td>\n",
       "      <td>7.568986e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540296</td>\n",
       "      <td>5.739230e-32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.785666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.950089e-03</td>\n",
       "      <td>2.239981e-04</td>\n",
       "      <td>2.343180e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528133</td>\n",
       "      <td>1.576703e-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.516644e-29</td>\n",
       "      <td>1.484843e-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1             2             3     4             5         6     \\\n",
       "0  0.069114   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "1  0.000000   0.0  4.804190e-09  7.041516e-10   0.0  2.004958e-12  0.000132   \n",
       "2  0.000000   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "3  0.785666   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "4  0.000000   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "\n",
       "           7             8             9         ...       2391  2392  2393  \\\n",
       "0  0.000000e+00  0.000000e+00  0.000000e+00      ...        0.0   0.0   0.0   \n",
       "1  1.046458e-07  5.863092e-16  7.568986e-23      ...        0.0   0.0   0.0   \n",
       "2  0.000000e+00  0.000000e+00  0.000000e+00      ...        0.0   0.0   0.0   \n",
       "3  1.950089e-03  2.239981e-04  2.343180e-07      ...        0.0   0.0   0.0   \n",
       "4  0.000000e+00  0.000000e+00  0.000000e+00      ...        0.0   0.0   0.0   \n",
       "\n",
       "   2394  2395      2396          2397  2398          2399          2400  \n",
       "0   0.0   0.0  0.000000  0.000000e+00   0.0  0.000000e+00  0.000000e+00  \n",
       "1   0.0   0.0  0.540296  5.739230e-32   0.0  0.000000e+00  0.000000e+00  \n",
       "2   0.0   0.0  0.000000  0.000000e+00   0.0  0.000000e+00  0.000000e+00  \n",
       "3   0.0   0.0  0.528133  1.576703e-09   0.0  2.516644e-29  1.484843e-57  \n",
       "4   0.0   0.0  0.000000  0.000000e+00   0.0  0.000000e+00  0.000000e+00  \n",
       "\n",
       "[5 rows x 2401 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding).to_csv(path_or_buf='tmp.csv', index=False)\n",
    "pd.DataFrame(embedding).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA on the embeddings\n",
    "The embeddings are sparse. We, therefore, apply PCA on the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9887812984792304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=35)\n",
    "pca.fit(embedding)\n",
    "X = pca.transform(embedding)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Multi-Layer Perceptron Classifier\n",
    "The PCA transforms of the embeddings are used directly as inputs to an MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73 samples\n",
      "Epoch 1/300\n",
      "73/73 [==============================] - 0s 6ms/sample - loss: 0.1492 - accuracy: 0.4384\n",
      "Epoch 2/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.1344 - accuracy: 0.5479\n",
      "Epoch 3/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.1307 - accuracy: 0.5753\n",
      "Epoch 4/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.1243 - accuracy: 0.7397\n",
      "Epoch 5/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.1287 - accuracy: 0.8082\n",
      "Epoch 6/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.1147 - accuracy: 0.8767\n",
      "Epoch 7/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.1050 - accuracy: 0.8493\n",
      "Epoch 8/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.1054 - accuracy: 0.8630\n",
      "Epoch 9/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.1016 - accuracy: 0.8767\n",
      "Epoch 10/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0920 - accuracy: 0.8767\n",
      "Epoch 11/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0886 - accuracy: 0.9178\n",
      "Epoch 12/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0941 - accuracy: 0.9178\n",
      "Epoch 13/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0911 - accuracy: 0.8904\n",
      "Epoch 14/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0800 - accuracy: 0.9178\n",
      "Epoch 15/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0752 - accuracy: 0.9178\n",
      "Epoch 16/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0710 - accuracy: 0.9178\n",
      "Epoch 17/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0689 - accuracy: 0.8904\n",
      "Epoch 18/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0765 - accuracy: 0.9178\n",
      "Epoch 19/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0687 - accuracy: 0.9178\n",
      "Epoch 20/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0629 - accuracy: 0.9178\n",
      "Epoch 21/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0622 - accuracy: 0.9178\n",
      "Epoch 22/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0635 - accuracy: 0.9178\n",
      "Epoch 23/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0659 - accuracy: 0.9041\n",
      "Epoch 24/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0605 - accuracy: 0.9041\n",
      "Epoch 25/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0641 - accuracy: 0.9178\n",
      "Epoch 26/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0564 - accuracy: 0.9178\n",
      "Epoch 27/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0526 - accuracy: 0.9178\n",
      "Epoch 28/300\n",
      "73/73 [==============================] - 0s 118us/sample - loss: 0.0576 - accuracy: 0.9041\n",
      "Epoch 29/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0524 - accuracy: 0.9178\n",
      "Epoch 30/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0551 - accuracy: 0.9178\n",
      "Epoch 31/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0468 - accuracy: 0.9178\n",
      "Epoch 32/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0449 - accuracy: 0.9178\n",
      "Epoch 33/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0445 - accuracy: 0.9178\n",
      "Epoch 34/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0466 - accuracy: 0.9178\n",
      "Epoch 35/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0479 - accuracy: 0.9178\n",
      "Epoch 36/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0393 - accuracy: 0.9178\n",
      "Epoch 37/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0472 - accuracy: 0.9178\n",
      "Epoch 38/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0393 - accuracy: 0.9178\n",
      "Epoch 39/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0414 - accuracy: 0.9178\n",
      "Epoch 40/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0381 - accuracy: 0.9178\n",
      "Epoch 41/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0420 - accuracy: 0.9178\n",
      "Epoch 42/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0400 - accuracy: 0.9178\n",
      "Epoch 43/300\n",
      "73/73 [==============================] - 0s 155us/sample - loss: 0.0398 - accuracy: 0.9178\n",
      "Epoch 44/300\n",
      "73/73 [==============================] - 0s 141us/sample - loss: 0.0355 - accuracy: 0.9178\n",
      "Epoch 45/300\n",
      "73/73 [==============================] - 0s 149us/sample - loss: 0.0342 - accuracy: 0.9178\n",
      "Epoch 46/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.86 - 0s 174us/sample - loss: 0.0401 - accuracy: 0.9178\n",
      "Epoch 47/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0389 - accuracy: 0.9178\n",
      "Epoch 48/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0329 - accuracy: 0.9178\n",
      "Epoch 49/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0382 - accuracy: 0.9178\n",
      "Epoch 50/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0338 - accuracy: 0.9178\n",
      "Epoch 51/300\n",
      "73/73 [==============================] - 0s 120us/sample - loss: 0.0325 - accuracy: 0.9178\n",
      "Epoch 52/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0388 - accuracy: 0.9178\n",
      "Epoch 53/300\n",
      "73/73 [==============================] - 0s 124us/sample - loss: 0.0369 - accuracy: 0.9178\n",
      "Epoch 54/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0388 - accuracy: 0.9178\n",
      "Epoch 55/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0354 - accuracy: 0.9178\n",
      "Epoch 56/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0352 - accuracy: 0.9178\n",
      "Epoch 57/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0293 - accuracy: 0.9178\n",
      "Epoch 58/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0324 - accuracy: 0.9178\n",
      "Epoch 59/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0321 - accuracy: 0.9178\n",
      "Epoch 60/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0376 - accuracy: 0.9178\n",
      "Epoch 61/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0328 - accuracy: 0.9178\n",
      "Epoch 62/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0320 - accuracy: 0.9178\n",
      "Epoch 63/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0346 - accuracy: 0.9178\n",
      "Epoch 64/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0271 - accuracy: 0.9178\n",
      "Epoch 65/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0320 - accuracy: 0.9178\n",
      "Epoch 66/300\n",
      "73/73 [==============================] - 0s 126us/sample - loss: 0.0293 - accuracy: 0.9178\n",
      "Epoch 67/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0304 - accuracy: 0.9178\n",
      "Epoch 68/300\n",
      "73/73 [==============================] - 0s 119us/sample - loss: 0.0288 - accuracy: 0.9315\n",
      "Epoch 69/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0286 - accuracy: 0.9178\n",
      "Epoch 70/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0282 - accuracy: 0.9178\n",
      "Epoch 71/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0300 - accuracy: 0.9178\n",
      "Epoch 72/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0299 - accuracy: 0.9178\n",
      "Epoch 73/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0280 - accuracy: 0.9178\n",
      "Epoch 74/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0296 - accuracy: 0.9178\n",
      "Epoch 75/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0266 - accuracy: 0.9178\n",
      "Epoch 76/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0263 - accuracy: 0.9178\n",
      "Epoch 77/300\n",
      "73/73 [==============================] - 0s 147us/sample - loss: 0.0233 - accuracy: 0.9178\n",
      "Epoch 78/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0327 - accuracy: 0.9178\n",
      "Epoch 79/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0280 - accuracy: 0.9178\n",
      "Epoch 80/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0286 - accuracy: 0.9178\n",
      "Epoch 81/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0279 - accuracy: 0.9178\n",
      "Epoch 82/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0285 - accuracy: 0.9178\n",
      "Epoch 83/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0308 - accuracy: 0.9178\n",
      "Epoch 84/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0275 - accuracy: 0.9178\n",
      "Epoch 85/300\n",
      "73/73 [==============================] - 0s 144us/sample - loss: 0.0277 - accuracy: 0.9178\n",
      "Epoch 86/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0272 - accuracy: 0.9178\n",
      "Epoch 87/300\n",
      "73/73 [==============================] - 0s 147us/sample - loss: 0.0269 - accuracy: 0.9178\n",
      "Epoch 88/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0269 - accuracy: 0.9178\n",
      "Epoch 89/300\n",
      "73/73 [==============================] - 0s 153us/sample - loss: 0.0252 - accuracy: 0.9178\n",
      "Epoch 90/300\n",
      "73/73 [==============================] - 0s 126us/sample - loss: 0.0244 - accuracy: 0.9178\n",
      "Epoch 91/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0281 - accuracy: 0.9315\n",
      "Epoch 92/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0284 - accuracy: 0.9178\n",
      "Epoch 93/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0280 - accuracy: 0.9315\n",
      "Epoch 94/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0248 - accuracy: 0.9178\n",
      "Epoch 95/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0191 - accuracy: 0.9178\n",
      "Epoch 96/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0260 - accuracy: 0.9178\n",
      "Epoch 97/300\n",
      "73/73 [==============================] - 0s 159us/sample - loss: 0.0242 - accuracy: 0.9178\n",
      "Epoch 98/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0241 - accuracy: 0.9178\n",
      "Epoch 99/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0268 - accuracy: 0.9178\n",
      "Epoch 100/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0235 - accuracy: 0.9178\n",
      "Epoch 101/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0251 - accuracy: 0.9178\n",
      "Epoch 102/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0265 - accuracy: 0.9178\n",
      "Epoch 103/300\n",
      "73/73 [==============================] - 0s 118us/sample - loss: 0.0254 - accuracy: 0.9178\n",
      "Epoch 104/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0236 - accuracy: 0.9178\n",
      "Epoch 105/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0195 - accuracy: 0.9178\n",
      "Epoch 106/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0244 - accuracy: 0.9178\n",
      "Epoch 107/300\n",
      "73/73 [==============================] - 0s 120us/sample - loss: 0.0214 - accuracy: 0.9178\n",
      "Epoch 108/300\n",
      "73/73 [==============================] - 0s 143us/sample - loss: 0.0270 - accuracy: 0.9178\n",
      "Epoch 109/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0219 - accuracy: 0.9178\n",
      "Epoch 110/300\n",
      "73/73 [==============================] - 0s 155us/sample - loss: 0.0244 - accuracy: 0.9178\n",
      "Epoch 111/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0218 - accuracy: 0.9178\n",
      "Epoch 112/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0268 - accuracy: 0.9178\n",
      "Epoch 113/300\n",
      "73/73 [==============================] - 0s 141us/sample - loss: 0.0209 - accuracy: 0.9178\n",
      "Epoch 114/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0214 - accuracy: 0.9178\n",
      "Epoch 115/300\n",
      "73/73 [==============================] - 0s 169us/sample - loss: 0.0287 - accuracy: 0.9041\n",
      "Epoch 116/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0267 - accuracy: 0.9178\n",
      "Epoch 117/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0265 - accuracy: 0.9178\n",
      "Epoch 118/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0265 - accuracy: 0.9315\n",
      "Epoch 119/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0284 - accuracy: 0.9178\n",
      "Epoch 120/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0221 - accuracy: 0.9178\n",
      "Epoch 121/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0244 - accuracy: 0.9178\n",
      "Epoch 122/300\n",
      "73/73 [==============================] - 0s 120us/sample - loss: 0.0217 - accuracy: 0.9178\n",
      "Epoch 123/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0241 - accuracy: 0.9178\n",
      "Epoch 124/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0241 - accuracy: 0.9178\n",
      "Epoch 125/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0242 - accuracy: 0.9178\n",
      "Epoch 126/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0245 - accuracy: 0.9315\n",
      "Epoch 127/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0255 - accuracy: 0.9178\n",
      "Epoch 128/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0260 - accuracy: 0.9178\n",
      "Epoch 129/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0233 - accuracy: 0.9178\n",
      "Epoch 130/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0211 - accuracy: 0.9178\n",
      "Epoch 131/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0244 - accuracy: 0.9178\n",
      "Epoch 132/300\n",
      "73/73 [==============================] - 0s 142us/sample - loss: 0.0206 - accuracy: 0.9178\n",
      "Epoch 133/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0239 - accuracy: 0.9178\n",
      "Epoch 134/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0229 - accuracy: 0.9178\n",
      "Epoch 135/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0223 - accuracy: 0.9178\n",
      "Epoch 136/300\n",
      "73/73 [==============================] - 0s 148us/sample - loss: 0.0263 - accuracy: 0.9178\n",
      "Epoch 137/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0255 - accuracy: 0.9178\n",
      "Epoch 138/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0221 - accuracy: 0.9178\n",
      "Epoch 139/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0229 - accuracy: 0.9178\n",
      "Epoch 140/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0257 - accuracy: 0.9178\n",
      "Epoch 141/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0208 - accuracy: 0.9178\n",
      "Epoch 142/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0205 - accuracy: 0.9178\n",
      "Epoch 143/300\n",
      "73/73 [==============================] - 0s 118us/sample - loss: 0.0210 - accuracy: 0.9178\n",
      "Epoch 144/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0240 - accuracy: 0.9178\n",
      "Epoch 145/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0293 - accuracy: 0.9178\n",
      "Epoch 146/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0238 - accuracy: 0.9178\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 145us/sample - loss: 0.0211 - accuracy: 0.9178\n",
      "Epoch 148/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0217 - accuracy: 0.9178\n",
      "Epoch 149/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0225 - accuracy: 0.9178\n",
      "Epoch 150/300\n",
      "73/73 [==============================] - 0s 117us/sample - loss: 0.0219 - accuracy: 0.9178\n",
      "Epoch 151/300\n",
      "73/73 [==============================] - 0s 145us/sample - loss: 0.0212 - accuracy: 0.9315\n",
      "Epoch 152/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0220 - accuracy: 0.9178\n",
      "Epoch 153/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0199 - accuracy: 0.9178\n",
      "Epoch 154/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0213 - accuracy: 0.9178\n",
      "Epoch 155/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0251 - accuracy: 0.9178\n",
      "Epoch 156/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0194 - accuracy: 0.9178\n",
      "Epoch 157/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0253 - accuracy: 0.9178\n",
      "Epoch 158/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0224 - accuracy: 0.9315\n",
      "Epoch 159/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0226 - accuracy: 0.9178\n",
      "Epoch 160/300\n",
      "73/73 [==============================] - 0s 120us/sample - loss: 0.0224 - accuracy: 0.9315\n",
      "Epoch 161/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0170 - accuracy: 0.9178\n",
      "Epoch 162/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0215 - accuracy: 0.9178\n",
      "Epoch 163/300\n",
      "73/73 [==============================] - 0s 141us/sample - loss: 0.0224 - accuracy: 0.9178\n",
      "Epoch 164/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0238 - accuracy: 0.9178\n",
      "Epoch 165/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0253 - accuracy: 0.9178\n",
      "Epoch 166/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0230 - accuracy: 0.9315\n",
      "Epoch 167/300\n",
      "73/73 [==============================] - 0s 154us/sample - loss: 0.0234 - accuracy: 0.9178\n",
      "Epoch 168/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0219 - accuracy: 0.9315\n",
      "Epoch 169/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0239 - accuracy: 0.9178\n",
      "Epoch 170/300\n",
      "73/73 [==============================] - 0s 144us/sample - loss: 0.0238 - accuracy: 0.9178\n",
      "Epoch 171/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0191 - accuracy: 0.9178\n",
      "Epoch 172/300\n",
      "73/73 [==============================] - 0s 148us/sample - loss: 0.0193 - accuracy: 0.9178\n",
      "Epoch 173/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0249 - accuracy: 0.9178\n",
      "Epoch 174/300\n",
      "73/73 [==============================] - 0s 142us/sample - loss: 0.0187 - accuracy: 0.9178\n",
      "Epoch 175/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0222 - accuracy: 0.9178\n",
      "Epoch 176/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0264 - accuracy: 0.9178\n",
      "Epoch 177/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.93 - 0s 125us/sample - loss: 0.0250 - accuracy: 0.9178\n",
      "Epoch 178/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0186 - accuracy: 0.9178\n",
      "Epoch 179/300\n",
      "73/73 [==============================] - 0s 124us/sample - loss: 0.0210 - accuracy: 0.9178\n",
      "Epoch 180/300\n",
      "73/73 [==============================] - 0s 119us/sample - loss: 0.0208 - accuracy: 0.9178\n",
      "Epoch 181/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0241 - accuracy: 0.9178\n",
      "Epoch 182/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0227 - accuracy: 0.9178\n",
      "Epoch 183/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - 0s 121us/sample - loss: 0.0227 - accuracy: 0.9178\n",
      "Epoch 184/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0202 - accuracy: 0.9178\n",
      "Epoch 185/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0203 - accuracy: 0.9178\n",
      "Epoch 186/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0192 - accuracy: 0.9178\n",
      "Epoch 187/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0208 - accuracy: 0.9178\n",
      "Epoch 188/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0206 - accuracy: 0.9178\n",
      "Epoch 189/300\n",
      "73/73 [==============================] - 0s 145us/sample - loss: 0.0172 - accuracy: 0.9315\n",
      "Epoch 190/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0192 - accuracy: 0.9178\n",
      "Epoch 191/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0215 - accuracy: 0.9315\n",
      "Epoch 192/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0197 - accuracy: 0.9315\n",
      "Epoch 193/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0242 - accuracy: 0.9178\n",
      "Epoch 194/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0242 - accuracy: 0.9178\n",
      "Epoch 195/300\n",
      "73/73 [==============================] - 0s 151us/sample - loss: 0.0277 - accuracy: 0.9041\n",
      "Epoch 196/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0214 - accuracy: 0.9178\n",
      "Epoch 197/300\n",
      "73/73 [==============================] - 0s 120us/sample - loss: 0.0298 - accuracy: 0.9178\n",
      "Epoch 198/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0215 - accuracy: 0.9178\n",
      "Epoch 199/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0193 - accuracy: 0.9178\n",
      "Epoch 200/300\n",
      "73/73 [==============================] - 0s 144us/sample - loss: 0.0230 - accuracy: 0.9178\n",
      "Epoch 201/300\n",
      "73/73 [==============================] - 0s 126us/sample - loss: 0.0191 - accuracy: 0.9178\n",
      "Epoch 202/300\n",
      "73/73 [==============================] - 0s 154us/sample - loss: 0.0223 - accuracy: 0.9178\n",
      "Epoch 203/300\n",
      "73/73 [==============================] - 0s 124us/sample - loss: 0.0218 - accuracy: 0.9178\n",
      "Epoch 204/300\n",
      "73/73 [==============================] - 0s 124us/sample - loss: 0.0185 - accuracy: 0.9178\n",
      "Epoch 205/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0196 - accuracy: 0.9178\n",
      "Epoch 206/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0224 - accuracy: 0.9178\n",
      "Epoch 207/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0231 - accuracy: 0.9178\n",
      "Epoch 208/300\n",
      "73/73 [==============================] - 0s 142us/sample - loss: 0.0187 - accuracy: 0.9178\n",
      "Epoch 209/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0182 - accuracy: 0.9315\n",
      "Epoch 210/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0249 - accuracy: 0.9178\n",
      "Epoch 211/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0253 - accuracy: 0.9178\n",
      "Epoch 212/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0210 - accuracy: 0.9178\n",
      "Epoch 213/300\n",
      "73/73 [==============================] - 0s 147us/sample - loss: 0.0207 - accuracy: 0.9315\n",
      "Epoch 214/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0237 - accuracy: 0.9178\n",
      "Epoch 215/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0178 - accuracy: 0.9315\n",
      "Epoch 216/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0230 - accuracy: 0.9178\n",
      "Epoch 217/300\n",
      "73/73 [==============================] - 0s 144us/sample - loss: 0.0217 - accuracy: 0.9178\n",
      "Epoch 218/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0223 - accuracy: 0.9178\n",
      "Epoch 219/300\n",
      "73/73 [==============================] - 0s 147us/sample - loss: 0.0182 - accuracy: 0.9178\n",
      "Epoch 220/300\n",
      "73/73 [==============================] - 0s 172us/sample - loss: 0.0229 - accuracy: 0.9178\n",
      "Epoch 221/300\n",
      "73/73 [==============================] - 0s 126us/sample - loss: 0.0187 - accuracy: 0.9178\n",
      "Epoch 222/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0223 - accuracy: 0.9178\n",
      "Epoch 223/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0224 - accuracy: 0.9315\n",
      "Epoch 224/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0255 - accuracy: 0.9178\n",
      "Epoch 225/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0219 - accuracy: 0.9178\n",
      "Epoch 226/300\n",
      "73/73 [==============================] - 0s 124us/sample - loss: 0.0178 - accuracy: 0.9178\n",
      "Epoch 227/300\n",
      "73/73 [==============================] - 0s 126us/sample - loss: 0.0241 - accuracy: 0.9178\n",
      "Epoch 228/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0215 - accuracy: 0.9178\n",
      "Epoch 229/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0180 - accuracy: 0.9178\n",
      "Epoch 230/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0202 - accuracy: 0.9178\n",
      "Epoch 231/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0213 - accuracy: 0.9178\n",
      "Epoch 232/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0203 - accuracy: 0.9178\n",
      "Epoch 233/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0285 - accuracy: 0.9041\n",
      "Epoch 234/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0198 - accuracy: 0.9315\n",
      "Epoch 235/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0226 - accuracy: 0.9178\n",
      "Epoch 236/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0225 - accuracy: 0.9178\n",
      "Epoch 237/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0251 - accuracy: 0.9178\n",
      "Epoch 238/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0222 - accuracy: 0.9178\n",
      "Epoch 239/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0215 - accuracy: 0.9178\n",
      "Epoch 240/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0203 - accuracy: 0.9178\n",
      "Epoch 241/300\n",
      "73/73 [==============================] - 0s 141us/sample - loss: 0.0214 - accuracy: 0.9178\n",
      "Epoch 242/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0233 - accuracy: 0.9178\n",
      "Epoch 243/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0227 - accuracy: 0.9178\n",
      "Epoch 244/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0233 - accuracy: 0.9178\n",
      "Epoch 245/300\n",
      "73/73 [==============================] - 0s 145us/sample - loss: 0.0200 - accuracy: 0.9178\n",
      "Epoch 246/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0250 - accuracy: 0.9178\n",
      "Epoch 247/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0229 - accuracy: 0.9178\n",
      "Epoch 248/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0201 - accuracy: 0.9178\n",
      "Epoch 249/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0192 - accuracy: 0.9178\n",
      "Epoch 250/300\n",
      "73/73 [==============================] - 0s 149us/sample - loss: 0.0224 - accuracy: 0.9178\n",
      "Epoch 251/300\n",
      "73/73 [==============================] - 0s 121us/sample - loss: 0.0192 - accuracy: 0.9178\n",
      "Epoch 252/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0236 - accuracy: 0.9041\n",
      "Epoch 253/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0213 - accuracy: 0.9178\n",
      "Epoch 254/300\n",
      "73/73 [==============================] - 0s 141us/sample - loss: 0.0243 - accuracy: 0.9178\n",
      "Epoch 255/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0248 - accuracy: 0.9178\n",
      "Epoch 256/300\n",
      "73/73 [==============================] - 0s 143us/sample - loss: 0.0212 - accuracy: 0.9178\n",
      "Epoch 257/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0199 - accuracy: 0.9178\n",
      "Epoch 258/300\n",
      "73/73 [==============================] - 0s 126us/sample - loss: 0.0219 - accuracy: 0.9178\n",
      "Epoch 259/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0252 - accuracy: 0.9178\n",
      "Epoch 260/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0227 - accuracy: 0.9178\n",
      "Epoch 261/300\n",
      "73/73 [==============================] - 0s 146us/sample - loss: 0.0219 - accuracy: 0.9178\n",
      "Epoch 262/300\n",
      "73/73 [==============================] - 0s 139us/sample - loss: 0.0218 - accuracy: 0.9178\n",
      "Epoch 263/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 5.4169e-04 - accuracy: 1.00 - 0s 126us/sample - loss: 0.0196 - accuracy: 0.9178\n",
      "Epoch 264/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0205 - accuracy: 0.9178\n",
      "Epoch 265/300\n",
      "73/73 [==============================] - 0s 131us/sample - loss: 0.0214 - accuracy: 0.9178\n",
      "Epoch 266/300\n",
      "73/73 [==============================] - 0s 141us/sample - loss: 0.0201 - accuracy: 0.9178\n",
      "Epoch 267/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0233 - accuracy: 0.9178\n",
      "Epoch 268/300\n",
      "73/73 [==============================] - 0s 122us/sample - loss: 0.0205 - accuracy: 0.9178\n",
      "Epoch 269/300\n",
      "73/73 [==============================] - 0s 138us/sample - loss: 0.0202 - accuracy: 0.9178\n",
      "Epoch 270/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0183 - accuracy: 0.9178\n",
      "Epoch 271/300\n",
      "73/73 [==============================] - 0s 148us/sample - loss: 0.0199 - accuracy: 0.9178\n",
      "Epoch 272/300\n",
      "73/73 [==============================] - 0s 123us/sample - loss: 0.0218 - accuracy: 0.9178\n",
      "Epoch 273/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0226 - accuracy: 0.9178\n",
      "Epoch 274/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0237 - accuracy: 0.9178\n",
      "Epoch 275/300\n",
      "73/73 [==============================] - 0s 135us/sample - loss: 0.0230 - accuracy: 0.9178\n",
      "Epoch 276/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0221 - accuracy: 0.9178\n",
      "Epoch 277/300\n",
      "73/73 [==============================] - 0s 148us/sample - loss: 0.0258 - accuracy: 0.9178\n",
      "Epoch 278/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0262 - accuracy: 0.9178\n",
      "Epoch 279/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0199 - accuracy: 0.9178\n",
      "Epoch 280/300\n",
      "73/73 [==============================] - 0s 134us/sample - loss: 0.0203 - accuracy: 0.9315\n",
      "Epoch 281/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0242 - accuracy: 0.9178\n",
      "Epoch 282/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.86 - 0s 145us/sample - loss: 0.0202 - accuracy: 0.9178\n",
      "Epoch 283/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0229 - accuracy: 0.9178\n",
      "Epoch 284/300\n",
      "73/73 [==============================] - 0s 125us/sample - loss: 0.0217 - accuracy: 0.9178\n",
      "Epoch 285/300\n",
      "73/73 [==============================] - 0s 146us/sample - loss: 0.0187 - accuracy: 0.9178\n",
      "Epoch 286/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0224 - accuracy: 0.9178\n",
      "Epoch 287/300\n",
      "73/73 [==============================] - 0s 149us/sample - loss: 0.0193 - accuracy: 0.9178\n",
      "Epoch 288/300\n",
      "73/73 [==============================] - 0s 129us/sample - loss: 0.0226 - accuracy: 0.9178\n",
      "Epoch 289/300\n",
      "73/73 [==============================] - 0s 136us/sample - loss: 0.0173 - accuracy: 0.9178\n",
      "Epoch 290/300\n",
      "73/73 [==============================] - 0s 128us/sample - loss: 0.0234 - accuracy: 0.9178\n",
      "Epoch 291/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0193 - accuracy: 0.9178\n",
      "Epoch 292/300\n",
      "73/73 [==============================] - 0s 133us/sample - loss: 0.0182 - accuracy: 0.9178\n",
      "Epoch 293/300\n",
      "73/73 [==============================] - 0s 130us/sample - loss: 0.0240 - accuracy: 0.9178\n",
      "Epoch 294/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0188 - accuracy: 0.9178\n",
      "Epoch 295/300\n",
      "73/73 [==============================] - 0s 127us/sample - loss: 0.0205 - accuracy: 0.9178\n",
      "Epoch 296/300\n",
      "73/73 [==============================] - 0s 137us/sample - loss: 0.0191 - accuracy: 0.9178\n",
      "Epoch 297/300\n",
      "73/73 [==============================] - 0s 147us/sample - loss: 0.0223 - accuracy: 0.9178\n",
      "Epoch 298/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0233 - accuracy: 0.9178\n",
      "Epoch 299/300\n",
      "73/73 [==============================] - 0s 140us/sample - loss: 0.0201 - accuracy: 0.9178\n",
      "Epoch 300/300\n",
      "73/73 [==============================] - 0s 132us/sample - loss: 0.0192 - accuracy: 0.9178\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_33 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n",
      "74/74 [==============================] - 0s 6ms/sample - loss: 0.1477 - accuracy: 0.6216\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.1468 - accuracy: 0.6351\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.1320 - accuracy: 0.7568\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.1203 - accuracy: 0.8108\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.1267 - accuracy: 0.7568\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.1173 - accuracy: 0.8108\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.1012 - accuracy: 0.8649\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.1027 - accuracy: 0.7838\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0976 - accuracy: 0.8649\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0892 - accuracy: 0.8514\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0876 - accuracy: 0.8784\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0829 - accuracy: 0.8514\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0828 - accuracy: 0.8784\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0802 - accuracy: 0.8919\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0853 - accuracy: 0.8649\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0761 - accuracy: 0.8784\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0679 - accuracy: 0.8919\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0650 - accuracy: 0.8919\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0628 - accuracy: 0.8919\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0669 - accuracy: 0.8919\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0584 - accuracy: 0.8784\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0616 - accuracy: 0.8784\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0565 - accuracy: 0.8919\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0650 - accuracy: 0.8784\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0514 - accuracy: 0.8784\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0509 - accuracy: 0.8919\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0485 - accuracy: 0.8919\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0496 - accuracy: 0.8784\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0513 - accuracy: 0.8919\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0537 - accuracy: 0.8784\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0511 - accuracy: 0.8919\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0446 - accuracy: 0.8919\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0448 - accuracy: 0.8919\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0438 - accuracy: 0.8919\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0424 - accuracy: 0.8919\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0422 - accuracy: 0.8919\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0446 - accuracy: 0.8919\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0425 - accuracy: 0.8919\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0397 - accuracy: 0.8919\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0399 - accuracy: 0.8919\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0423 - accuracy: 0.8919\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0406 - accuracy: 0.8919\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0420 - accuracy: 0.8784\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0370 - accuracy: 0.8919\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0367 - accuracy: 0.8919\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0379 - accuracy: 0.8919\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0410 - accuracy: 0.8919\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0351 - accuracy: 0.8919\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0335 - accuracy: 0.8919\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0362 - accuracy: 0.8919\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0368 - accuracy: 0.8919\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0367 - accuracy: 0.8919\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0339 - accuracy: 0.8919\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0334 - accuracy: 0.8919\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0371 - accuracy: 0.8919\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0350 - accuracy: 0.8919\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0350 - accuracy: 0.8919\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0320 - accuracy: 0.8919\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0378 - accuracy: 0.8919\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0368 - accuracy: 0.8919\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0355 - accuracy: 0.8919\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0347 - accuracy: 0.8919\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0318 - accuracy: 0.8919\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0320 - accuracy: 0.8919\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0338 - accuracy: 0.8919\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0388 - accuracy: 0.8784\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0325 - accuracy: 0.8919\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0315 - accuracy: 0.8919\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0336 - accuracy: 0.8919\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0307 - accuracy: 0.8919\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0319 - accuracy: 0.8919\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0362 - accuracy: 0.8919\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0330 - accuracy: 0.8919\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0316 - accuracy: 0.8919\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0333 - accuracy: 0.8919\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0360 - accuracy: 0.8919\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0317 - accuracy: 0.8919\n",
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0359 - accuracy: 0.8919\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0309 - accuracy: 0.8919\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 178us/sample - loss: 0.0343 - accuracy: 0.8919\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0319 - accuracy: 0.8919\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0317 - accuracy: 0.8919\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0346 - accuracy: 0.8919\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0281 - accuracy: 0.8919\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0347 - accuracy: 0.8919\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0318 - accuracy: 0.8919\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0327 - accuracy: 0.8919\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0279 - accuracy: 0.8919\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0313 - accuracy: 0.8919\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0326 - accuracy: 0.8919\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0332 - accuracy: 0.8919\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0321 - accuracy: 0.8919\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0276 - accuracy: 0.8919\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0346 - accuracy: 0.8919\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0348 - accuracy: 0.8919\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0337 - accuracy: 0.8919\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0329 - accuracy: 0.8919\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0283 - accuracy: 0.8919\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0328 - accuracy: 0.8919\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0325 - accuracy: 0.8919\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0291 - accuracy: 0.8919\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0351 - accuracy: 0.8784\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0338 - accuracy: 0.8919\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0327 - accuracy: 0.8919\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0305 - accuracy: 0.8919\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0328 - accuracy: 0.8919\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0307 - accuracy: 0.8919\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0280 - accuracy: 0.8919\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0349 - accuracy: 0.8919\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0286 - accuracy: 0.8919\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0292 - accuracy: 0.8919\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0307 - accuracy: 0.8919\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0297 - accuracy: 0.8919\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0311 - accuracy: 0.8919\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0269 - accuracy: 0.8919\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0309 - accuracy: 0.8919\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0288 - accuracy: 0.8919\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0288 - accuracy: 0.8919\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0287 - accuracy: 0.8919\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0310 - accuracy: 0.8919\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0283 - accuracy: 0.8919\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0289 - accuracy: 0.8919\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0333 - accuracy: 0.8919\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0310 - accuracy: 0.8919\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0331 - accuracy: 0.8919\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0342 - accuracy: 0.8919\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0284 - accuracy: 0.8919\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0274 - accuracy: 0.8919\n",
      "Epoch 147/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0318 - accuracy: 0.8919\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0271 - accuracy: 0.8919\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0286 - accuracy: 0.8919\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0298 - accuracy: 0.8919\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0319 - accuracy: 0.8919\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0314 - accuracy: 0.8919\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0260 - accuracy: 0.8919\n",
      "Epoch 156/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0319 - accuracy: 0.8919\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0308 - accuracy: 0.8919\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0279 - accuracy: 0.8919\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0281 - accuracy: 0.8919\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0281 - accuracy: 0.8919\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0313 - accuracy: 0.8919\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0278 - accuracy: 0.8919\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0305 - accuracy: 0.8919\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0248 - accuracy: 0.8919\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0282 - accuracy: 0.8919\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0350 - accuracy: 0.8919\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0300 - accuracy: 0.8919\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0252 - accuracy: 0.8919\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0300 - accuracy: 0.8919\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0307 - accuracy: 0.8919\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0283 - accuracy: 0.8919\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0294 - accuracy: 0.8919\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0318 - accuracy: 0.8919\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0349 - accuracy: 0.8919\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0261 - accuracy: 0.8919\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0309 - accuracy: 0.8919\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0317 - accuracy: 0.8919\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0292 - accuracy: 0.8919\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0254 - accuracy: 0.8919\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 167us/sample - loss: 0.0360 - accuracy: 0.8919\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 175us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 176us/sample - loss: 0.0278 - accuracy: 0.8919\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 164us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0289 - accuracy: 0.8919\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0317 - accuracy: 0.8919\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0308 - accuracy: 0.8919\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0296 - accuracy: 0.8919\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0282 - accuracy: 0.8919\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0280 - accuracy: 0.8919\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0327 - accuracy: 0.8919\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0294 - accuracy: 0.8919\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0343 - accuracy: 0.8919\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0289 - accuracy: 0.8919\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0281 - accuracy: 0.8919\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0283 - accuracy: 0.8919\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0311 - accuracy: 0.8919\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0259 - accuracy: 0.9054\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 155us/sample - loss: 0.0356 - accuracy: 0.8919\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0284 - accuracy: 0.8919\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0290 - accuracy: 0.8919\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0298 - accuracy: 0.8919\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 166us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0270 - accuracy: 0.8919\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0312 - accuracy: 0.8919\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0310 - accuracy: 0.8919\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0287 - accuracy: 0.8919\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0319 - accuracy: 0.8919\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0348 - accuracy: 0.8919\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0290 - accuracy: 0.8919\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0279 - accuracy: 0.8919\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0261 - accuracy: 0.8919\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0285 - accuracy: 0.8919\n",
      "Epoch 231/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0289 - accuracy: 0.8919\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0303 - accuracy: 0.8919\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0300 - accuracy: 0.8919\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0320 - accuracy: 0.8919\n",
      "Epoch 236/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0308 - accuracy: 0.8919\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0289 - accuracy: 0.8919\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0269 - accuracy: 0.8919\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0287 - accuracy: 0.8919\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0341 - accuracy: 0.8919\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0287 - accuracy: 0.8919\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0285 - accuracy: 0.8919\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0269 - accuracy: 0.8919\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0287 - accuracy: 0.8919\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0274 - accuracy: 0.8919\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0316 - accuracy: 0.8919\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0266 - accuracy: 0.8919\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0281 - accuracy: 0.8919\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0278 - accuracy: 0.8919\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0255 - accuracy: 0.8919\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0263 - accuracy: 0.8919\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0276 - accuracy: 0.8919\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0284 - accuracy: 0.8919\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0260 - accuracy: 0.8919\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0270 - accuracy: 0.8919\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0316 - accuracy: 0.8919\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0285 - accuracy: 0.8919\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0286 - accuracy: 0.8919\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0302 - accuracy: 0.8919\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0282 - accuracy: 0.8919\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0303 - accuracy: 0.8919\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0277 - accuracy: 0.8919\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0285 - accuracy: 0.8919\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0300 - accuracy: 0.8919\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0262 - accuracy: 0.8919\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.7768e-04 - accuracy: 1.00 - 0s 159us/sample - loss: 0.0264 - accuracy: 0.8919\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0287 - accuracy: 0.8919\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0236 - accuracy: 0.8919\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0328 - accuracy: 0.8919\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0286 - accuracy: 0.8919\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0265 - accuracy: 0.8919\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0269 - accuracy: 0.8919\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0311 - accuracy: 0.8919\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0292 - accuracy: 0.8919\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0272 - accuracy: 0.8919\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0292 - accuracy: 0.8919\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0275 - accuracy: 0.8919\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0291 - accuracy: 0.8919\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0294 - accuracy: 0.8919\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0242 - accuracy: 0.8919\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0301 - accuracy: 0.8919\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0307 - accuracy: 0.8919\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0291 - accuracy: 0.8919\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0249 - accuracy: 0.8919\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0318 - accuracy: 0.8919\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0339 - accuracy: 0.8919\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0293 - accuracy: 0.8919\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 1)                 0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 75 samples\n",
      "Epoch 1/300\n",
      "75/75 [==============================] - 0s 4ms/sample - loss: 0.1324 - accuracy: 0.5867\n",
      "Epoch 2/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.1279 - accuracy: 0.7067\n",
      "Epoch 3/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.1212 - accuracy: 0.6933\n",
      "Epoch 4/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.1169 - accuracy: 0.7467\n",
      "Epoch 5/300\n",
      "75/75 [==============================] - 0s 107us/sample - loss: 0.1173 - accuracy: 0.8000\n",
      "Epoch 6/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.1112 - accuracy: 0.7467\n",
      "Epoch 7/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.1108 - accuracy: 0.8533\n",
      "Epoch 8/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.1091 - accuracy: 0.7600\n",
      "Epoch 9/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0986 - accuracy: 0.8267\n",
      "Epoch 10/300\n",
      "75/75 [==============================] - 0s 101us/sample - loss: 0.0989 - accuracy: 0.8000\n",
      "Epoch 11/300\n",
      "75/75 [==============================] - 0s 112us/sample - loss: 0.0915 - accuracy: 0.8400\n",
      "Epoch 12/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0899 - accuracy: 0.8133\n",
      "Epoch 13/300\n",
      "75/75 [==============================] - 0s 105us/sample - loss: 0.0857 - accuracy: 0.8400\n",
      "Epoch 14/300\n",
      "75/75 [==============================] - 0s 115us/sample - loss: 0.0869 - accuracy: 0.8267\n",
      "Epoch 15/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0902 - accuracy: 0.8267\n",
      "Epoch 16/300\n",
      "75/75 [==============================] - 0s 103us/sample - loss: 0.0822 - accuracy: 0.8400\n",
      "Epoch 17/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0778 - accuracy: 0.8533\n",
      "Epoch 18/300\n",
      "75/75 [==============================] - 0s 106us/sample - loss: 0.0859 - accuracy: 0.8133\n",
      "Epoch 19/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0715 - accuracy: 0.8400\n",
      "Epoch 20/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0713 - accuracy: 0.8533\n",
      "Epoch 21/300\n",
      "75/75 [==============================] - 0s 103us/sample - loss: 0.0691 - accuracy: 0.8533\n",
      "Epoch 22/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0699 - accuracy: 0.8533\n",
      "Epoch 23/300\n",
      "75/75 [==============================] - 0s 119us/sample - loss: 0.0695 - accuracy: 0.8533\n",
      "Epoch 24/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0626 - accuracy: 0.8533\n",
      "Epoch 25/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0694 - accuracy: 0.8533\n",
      "Epoch 26/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0637 - accuracy: 0.8533\n",
      "Epoch 27/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.86 - 0s 112us/sample - loss: 0.0631 - accuracy: 0.8533\n",
      "Epoch 28/300\n",
      "75/75 [==============================] - 0s 106us/sample - loss: 0.0702 - accuracy: 0.8533\n",
      "Epoch 29/300\n",
      "75/75 [==============================] - 0s 106us/sample - loss: 0.0646 - accuracy: 0.8400\n",
      "Epoch 30/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0540 - accuracy: 0.8533\n",
      "Epoch 31/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0641 - accuracy: 0.8533\n",
      "Epoch 32/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0611 - accuracy: 0.8533\n",
      "Epoch 33/300\n",
      "75/75 [==============================] - 0s 106us/sample - loss: 0.0583 - accuracy: 0.8533\n",
      "Epoch 34/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0584 - accuracy: 0.8533\n",
      "Epoch 35/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0550 - accuracy: 0.8400\n",
      "Epoch 36/300\n",
      "75/75 [==============================] - 0s 153us/sample - loss: 0.0587 - accuracy: 0.8533\n",
      "Epoch 37/300\n",
      "75/75 [==============================] - 0s 143us/sample - loss: 0.0506 - accuracy: 0.8533\n",
      "Epoch 38/300\n",
      "75/75 [==============================] - 0s 166us/sample - loss: 0.0555 - accuracy: 0.8533\n",
      "Epoch 39/300\n",
      "75/75 [==============================] - 0s 126us/sample - loss: 0.0532 - accuracy: 0.8533\n",
      "Epoch 40/300\n",
      "75/75 [==============================] - 0s 184us/sample - loss: 0.0542 - accuracy: 0.8533\n",
      "Epoch 41/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0525 - accuracy: 0.8533\n",
      "Epoch 42/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0522 - accuracy: 0.8533\n",
      "Epoch 43/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0511 - accuracy: 0.8533\n",
      "Epoch 44/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0541 - accuracy: 0.8533\n",
      "Epoch 45/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0510 - accuracy: 0.8400\n",
      "Epoch 46/300\n",
      "75/75 [==============================] - 0s 110us/sample - loss: 0.0495 - accuracy: 0.8533\n",
      "Epoch 47/300\n",
      "75/75 [==============================] - 0s 110us/sample - loss: 0.0455 - accuracy: 0.8533\n",
      "Epoch 48/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0464 - accuracy: 0.8533\n",
      "Epoch 49/300\n",
      "75/75 [==============================] - 0s 112us/sample - loss: 0.0488 - accuracy: 0.8533\n",
      "Epoch 50/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0454 - accuracy: 0.8533\n",
      "Epoch 51/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0478 - accuracy: 0.8533\n",
      "Epoch 52/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0467 - accuracy: 0.8533\n",
      "Epoch 53/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0490 - accuracy: 0.8533\n",
      "Epoch 54/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.73 - 0s 130us/sample - loss: 0.0466 - accuracy: 0.8533\n",
      "Epoch 55/300\n",
      "75/75 [==============================] - 0s 142us/sample - loss: 0.0481 - accuracy: 0.8533\n",
      "Epoch 56/300\n",
      "75/75 [==============================] - 0s 146us/sample - loss: 0.0480 - accuracy: 0.8533\n",
      "Epoch 57/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0496 - accuracy: 0.8533\n",
      "Epoch 58/300\n",
      "75/75 [==============================] - 0s 126us/sample - loss: 0.0453 - accuracy: 0.8533\n",
      "Epoch 59/300\n",
      "75/75 [==============================] - 0s 135us/sample - loss: 0.0439 - accuracy: 0.8533\n",
      "Epoch 60/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0445 - accuracy: 0.8533\n",
      "Epoch 61/300\n",
      "75/75 [==============================] - 0s 138us/sample - loss: 0.0442 - accuracy: 0.8533\n",
      "Epoch 62/300\n",
      "75/75 [==============================] - 0s 153us/sample - loss: 0.0512 - accuracy: 0.8667\n",
      "Epoch 63/300\n",
      "75/75 [==============================] - 0s 129us/sample - loss: 0.0399 - accuracy: 0.8533\n",
      "Epoch 64/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0459 - accuracy: 0.8533\n",
      "Epoch 65/300\n",
      "75/75 [==============================] - 0s 128us/sample - loss: 0.0423 - accuracy: 0.8533\n",
      "Epoch 66/300\n",
      "75/75 [==============================] - 0s 134us/sample - loss: 0.0469 - accuracy: 0.8533\n",
      "Epoch 67/300\n",
      "75/75 [==============================] - 0s 154us/sample - loss: 0.0437 - accuracy: 0.8533\n",
      "Epoch 68/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0446 - accuracy: 0.8533\n",
      "Epoch 69/300\n",
      "75/75 [==============================] - 0s 138us/sample - loss: 0.0404 - accuracy: 0.8533\n",
      "Epoch 70/300\n",
      "75/75 [==============================] - 0s 131us/sample - loss: 0.0442 - accuracy: 0.8533\n",
      "Epoch 71/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0435 - accuracy: 0.8533\n",
      "Epoch 72/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0418 - accuracy: 0.8533\n",
      "Epoch 73/300\n",
      "75/75 [==============================] - 0s 138us/sample - loss: 0.0463 - accuracy: 0.8533\n",
      "Epoch 74/300\n",
      "75/75 [==============================] - 0s 143us/sample - loss: 0.0513 - accuracy: 0.8533\n",
      "Epoch 75/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0462 - accuracy: 0.8667\n",
      "Epoch 76/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0413 - accuracy: 0.8533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300\n",
      "75/75 [==============================] - 0s 131us/sample - loss: 0.0410 - accuracy: 0.8533\n",
      "Epoch 78/300\n",
      "75/75 [==============================] - 0s 188us/sample - loss: 0.0416 - accuracy: 0.8533\n",
      "Epoch 79/300\n",
      "75/75 [==============================] - 0s 144us/sample - loss: 0.0445 - accuracy: 0.8533\n",
      "Epoch 80/300\n",
      "75/75 [==============================] - 0s 158us/sample - loss: 0.0430 - accuracy: 0.8533\n",
      "Epoch 81/300\n",
      "75/75 [==============================] - 0s 137us/sample - loss: 0.0374 - accuracy: 0.8533\n",
      "Epoch 82/300\n",
      "75/75 [==============================] - 0s 119us/sample - loss: 0.0466 - accuracy: 0.8533\n",
      "Epoch 83/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0396 - accuracy: 0.8533\n",
      "Epoch 84/300\n",
      "75/75 [==============================] - 0s 173us/sample - loss: 0.0459 - accuracy: 0.8533\n",
      "Epoch 85/300\n",
      "75/75 [==============================] - 0s 152us/sample - loss: 0.0483 - accuracy: 0.8533\n",
      "Epoch 86/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0441 - accuracy: 0.8533\n",
      "Epoch 87/300\n",
      "75/75 [==============================] - 0s 154us/sample - loss: 0.0389 - accuracy: 0.8533\n",
      "Epoch 88/300\n",
      "75/75 [==============================] - 0s 150us/sample - loss: 0.0416 - accuracy: 0.8533\n",
      "Epoch 89/300\n",
      "75/75 [==============================] - 0s 128us/sample - loss: 0.0452 - accuracy: 0.8533\n",
      "Epoch 90/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0459 - accuracy: 0.8533\n",
      "Epoch 91/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0424 - accuracy: 0.8533\n",
      "Epoch 92/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0381 - accuracy: 0.8533\n",
      "Epoch 93/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0421 - accuracy: 0.8533\n",
      "Epoch 94/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0369 - accuracy: 0.8533\n",
      "Epoch 95/300\n",
      "75/75 [==============================] - 0s 119us/sample - loss: 0.0423 - accuracy: 0.8533\n",
      "Epoch 96/300\n",
      "75/75 [==============================] - 0s 131us/sample - loss: 0.0358 - accuracy: 0.8667\n",
      "Epoch 97/300\n",
      "75/75 [==============================] - 0s 134us/sample - loss: 0.0405 - accuracy: 0.8533\n",
      "Epoch 98/300\n",
      "75/75 [==============================] - 0s 129us/sample - loss: 0.0374 - accuracy: 0.8533\n",
      "Epoch 99/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0378 - accuracy: 0.8533\n",
      "Epoch 100/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0410 - accuracy: 0.8533\n",
      "Epoch 101/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0418 - accuracy: 0.8533\n",
      "Epoch 102/300\n",
      "75/75 [==============================] - 0s 131us/sample - loss: 0.0463 - accuracy: 0.8533\n",
      "Epoch 103/300\n",
      "75/75 [==============================] - 0s 129us/sample - loss: 0.0429 - accuracy: 0.8533\n",
      "Epoch 104/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0408 - accuracy: 0.8667\n",
      "Epoch 105/300\n",
      "75/75 [==============================] - 0s 131us/sample - loss: 0.0359 - accuracy: 0.8533\n",
      "Epoch 106/300\n",
      "75/75 [==============================] - 0s 145us/sample - loss: 0.0404 - accuracy: 0.8667\n",
      "Epoch 107/300\n",
      "75/75 [==============================] - 0s 162us/sample - loss: 0.0367 - accuracy: 0.8533\n",
      "Epoch 108/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0407 - accuracy: 0.8533\n",
      "Epoch 109/300\n",
      "75/75 [==============================] - 0s 149us/sample - loss: 0.0417 - accuracy: 0.8533\n",
      "Epoch 110/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0395 - accuracy: 0.8533\n",
      "Epoch 111/300\n",
      "75/75 [==============================] - 0s 129us/sample - loss: 0.0391 - accuracy: 0.8533\n",
      "Epoch 112/300\n",
      "75/75 [==============================] - 0s 135us/sample - loss: 0.0441 - accuracy: 0.8533\n",
      "Epoch 113/300\n",
      "75/75 [==============================] - 0s 135us/sample - loss: 0.0411 - accuracy: 0.8533\n",
      "Epoch 114/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0372 - accuracy: 0.8533\n",
      "Epoch 115/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0381 - accuracy: 0.8533\n",
      "Epoch 116/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0409 - accuracy: 0.8533\n",
      "Epoch 117/300\n",
      "75/75 [==============================] - 0s 142us/sample - loss: 0.0400 - accuracy: 0.8533\n",
      "Epoch 118/300\n",
      "75/75 [==============================] - 0s 159us/sample - loss: 0.0394 - accuracy: 0.8533\n",
      "Epoch 119/300\n",
      "75/75 [==============================] - 0s 144us/sample - loss: 0.0453 - accuracy: 0.8533\n",
      "Epoch 120/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0353 - accuracy: 0.8533\n",
      "Epoch 121/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0375 - accuracy: 0.8533\n",
      "Epoch 122/300\n",
      "75/75 [==============================] - 0s 151us/sample - loss: 0.0391 - accuracy: 0.8533\n",
      "Epoch 123/300\n",
      "75/75 [==============================] - 0s 202us/sample - loss: 0.0383 - accuracy: 0.8533\n",
      "Epoch 124/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0394 - accuracy: 0.8533\n",
      "Epoch 125/300\n",
      "75/75 [==============================] - 0s 128us/sample - loss: 0.0361 - accuracy: 0.8533\n",
      "Epoch 126/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0336 - accuracy: 0.8533\n",
      "Epoch 127/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0374 - accuracy: 0.8533\n",
      "Epoch 128/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0425 - accuracy: 0.8533\n",
      "Epoch 129/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.86 - 0s 118us/sample - loss: 0.0376 - accuracy: 0.8533\n",
      "Epoch 130/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0386 - accuracy: 0.8533\n",
      "Epoch 131/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0385 - accuracy: 0.8533\n",
      "Epoch 132/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0367 - accuracy: 0.8533\n",
      "Epoch 133/300\n",
      "75/75 [==============================] - 0s 118us/sample - loss: 0.0397 - accuracy: 0.8533\n",
      "Epoch 134/300\n",
      "75/75 [==============================] - 0s 118us/sample - loss: 0.0394 - accuracy: 0.8533\n",
      "Epoch 135/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0431 - accuracy: 0.8533\n",
      "Epoch 136/300\n",
      "75/75 [==============================] - 0s 104us/sample - loss: 0.0370 - accuracy: 0.8533\n",
      "Epoch 137/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0415 - accuracy: 0.8533\n",
      "Epoch 138/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0393 - accuracy: 0.8533\n",
      "Epoch 139/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0404 - accuracy: 0.8533\n",
      "Epoch 140/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0421 - accuracy: 0.8533\n",
      "Epoch 141/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0400 - accuracy: 0.8533\n",
      "Epoch 142/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0368 - accuracy: 0.8533\n",
      "Epoch 143/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0395 - accuracy: 0.8533\n",
      "Epoch 144/300\n",
      "75/75 [==============================] - 0s 149us/sample - loss: 0.0379 - accuracy: 0.8533\n",
      "Epoch 145/300\n",
      "75/75 [==============================] - 0s 160us/sample - loss: 0.0398 - accuracy: 0.8533\n",
      "Epoch 146/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0407 - accuracy: 0.8533\n",
      "Epoch 147/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0402 - accuracy: 0.8533\n",
      "Epoch 148/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0350 - accuracy: 0.8533\n",
      "Epoch 149/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0388 - accuracy: 0.8533\n",
      "Epoch 150/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0371 - accuracy: 0.8533\n",
      "Epoch 151/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0438 - accuracy: 0.8533\n",
      "Epoch 152/300\n",
      "75/75 [==============================] - 0s 140us/sample - loss: 0.0399 - accuracy: 0.8533\n",
      "Epoch 153/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0387 - accuracy: 0.8533\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 126us/sample - loss: 0.0390 - accuracy: 0.8533\n",
      "Epoch 155/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0380 - accuracy: 0.8533\n",
      "Epoch 156/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0377 - accuracy: 0.8533\n",
      "Epoch 157/300\n",
      "75/75 [==============================] - 0s 164us/sample - loss: 0.0387 - accuracy: 0.8533\n",
      "Epoch 158/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 159/300\n",
      "75/75 [==============================] - 0s 115us/sample - loss: 0.0405 - accuracy: 0.8533\n",
      "Epoch 160/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0362 - accuracy: 0.8533\n",
      "Epoch 161/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0385 - accuracy: 0.8533\n",
      "Epoch 162/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0360 - accuracy: 0.8533\n",
      "Epoch 163/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0405 - accuracy: 0.8533\n",
      "Epoch 164/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0400 - accuracy: 0.8533\n",
      "Epoch 165/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0398 - accuracy: 0.8533\n",
      "Epoch 166/300\n",
      "75/75 [==============================] - 0s 150us/sample - loss: 0.0386 - accuracy: 0.8533\n",
      "Epoch 167/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0369 - accuracy: 0.8533\n",
      "Epoch 168/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0378 - accuracy: 0.8533\n",
      "Epoch 169/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0369 - accuracy: 0.8533\n",
      "Epoch 170/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0419 - accuracy: 0.8533\n",
      "Epoch 171/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0394 - accuracy: 0.8533\n",
      "Epoch 172/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0373 - accuracy: 0.8533\n",
      "Epoch 173/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0395 - accuracy: 0.8533\n",
      "Epoch 174/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0360 - accuracy: 0.8533\n",
      "Epoch 175/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0394 - accuracy: 0.8533\n",
      "Epoch 176/300\n",
      "75/75 [==============================] - 0s 105us/sample - loss: 0.0401 - accuracy: 0.8533\n",
      "Epoch 177/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 178/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0386 - accuracy: 0.8533\n",
      "Epoch 179/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0381 - accuracy: 0.8533\n",
      "Epoch 180/300\n",
      "75/75 [==============================] - 0s 126us/sample - loss: 0.0393 - accuracy: 0.8533\n",
      "Epoch 181/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0388 - accuracy: 0.8533\n",
      "Epoch 182/300\n",
      "75/75 [==============================] - 0s 118us/sample - loss: 0.0441 - accuracy: 0.8533\n",
      "Epoch 183/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0409 - accuracy: 0.8533\n",
      "Epoch 184/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0383 - accuracy: 0.8533\n",
      "Epoch 185/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0333 - accuracy: 0.8533\n",
      "Epoch 186/300\n",
      "75/75 [==============================] - 0s 104us/sample - loss: 0.0383 - accuracy: 0.8533\n",
      "Epoch 187/300\n",
      "75/75 [==============================] - 0s 115us/sample - loss: 0.0346 - accuracy: 0.8533\n",
      "Epoch 188/300\n",
      "75/75 [==============================] - 0s 128us/sample - loss: 0.0352 - accuracy: 0.8533\n",
      "Epoch 189/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0400 - accuracy: 0.8533\n",
      "Epoch 190/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0352 - accuracy: 0.8533\n",
      "Epoch 191/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0375 - accuracy: 0.8533\n",
      "Epoch 192/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 193/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0364 - accuracy: 0.8533\n",
      "Epoch 194/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0387 - accuracy: 0.8533\n",
      "Epoch 195/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0399 - accuracy: 0.8533\n",
      "Epoch 196/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0366 - accuracy: 0.8533\n",
      "Epoch 197/300\n",
      "75/75 [==============================] - 0s 112us/sample - loss: 0.0389 - accuracy: 0.8533\n",
      "Epoch 198/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0366 - accuracy: 0.8533\n",
      "Epoch 199/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0391 - accuracy: 0.8533\n",
      "Epoch 200/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0355 - accuracy: 0.8533\n",
      "Epoch 201/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0373 - accuracy: 0.8533\n",
      "Epoch 202/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0375 - accuracy: 0.8533\n",
      "Epoch 203/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0392 - accuracy: 0.8533\n",
      "Epoch 204/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0362 - accuracy: 0.8533\n",
      "Epoch 205/300\n",
      "75/75 [==============================] - 0s 118us/sample - loss: 0.0374 - accuracy: 0.8533\n",
      "Epoch 206/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0422 - accuracy: 0.8533\n",
      "Epoch 207/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0400 - accuracy: 0.8533\n",
      "Epoch 208/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0407 - accuracy: 0.8533\n",
      "Epoch 209/300\n",
      "75/75 [==============================] - 0s 131us/sample - loss: 0.0374 - accuracy: 0.8533\n",
      "Epoch 210/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0402 - accuracy: 0.8533\n",
      "Epoch 211/300\n",
      "75/75 [==============================] - 0s 137us/sample - loss: 0.0377 - accuracy: 0.8533\n",
      "Epoch 212/300\n",
      "75/75 [==============================] - 0s 115us/sample - loss: 0.0339 - accuracy: 0.8533\n",
      "Epoch 213/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0414 - accuracy: 0.8400\n",
      "Epoch 214/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0350 - accuracy: 0.8533\n",
      "Epoch 215/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0425 - accuracy: 0.8533\n",
      "Epoch 216/300\n",
      "75/75 [==============================] - 0s 146us/sample - loss: 0.0401 - accuracy: 0.8533\n",
      "Epoch 217/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 218/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0380 - accuracy: 0.8533\n",
      "Epoch 219/300\n",
      "75/75 [==============================] - 0s 119us/sample - loss: 0.0372 - accuracy: 0.8533\n",
      "Epoch 220/300\n",
      "75/75 [==============================] - 0s 143us/sample - loss: 0.0347 - accuracy: 0.8533\n",
      "Epoch 221/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 222/300\n",
      "75/75 [==============================] - 0s 129us/sample - loss: 0.0369 - accuracy: 0.8533\n",
      "Epoch 223/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0397 - accuracy: 0.8533\n",
      "Epoch 224/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0346 - accuracy: 0.8533\n",
      "Epoch 225/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0385 - accuracy: 0.8533\n",
      "Epoch 226/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0402 - accuracy: 0.8533\n",
      "Epoch 227/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0380 - accuracy: 0.8533\n",
      "Epoch 228/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0394 - accuracy: 0.8533\n",
      "Epoch 229/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0362 - accuracy: 0.8533\n",
      "Epoch 230/300\n",
      "75/75 [==============================] - 0s 120us/sample - loss: 0.0396 - accuracy: 0.8667\n",
      "Epoch 231/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0402 - accuracy: 0.8533\n",
      "Epoch 232/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 110us/sample - loss: 0.0388 - accuracy: 0.8533\n",
      "Epoch 233/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0377 - accuracy: 0.8533\n",
      "Epoch 234/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0362 - accuracy: 0.8533\n",
      "Epoch 235/300\n",
      "75/75 [==============================] - 0s 129us/sample - loss: 0.0332 - accuracy: 0.8533\n",
      "Epoch 236/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0361 - accuracy: 0.8533\n",
      "Epoch 237/300\n",
      "75/75 [==============================] - 0s 145us/sample - loss: 0.0404 - accuracy: 0.8533\n",
      "Epoch 238/300\n",
      "75/75 [==============================] - 0s 143us/sample - loss: 0.0368 - accuracy: 0.8533\n",
      "Epoch 239/300\n",
      "75/75 [==============================] - 0s 112us/sample - loss: 0.0362 - accuracy: 0.8533\n",
      "Epoch 240/300\n",
      "75/75 [==============================] - 0s 141us/sample - loss: 0.0409 - accuracy: 0.8667\n",
      "Epoch 241/300\n",
      "75/75 [==============================] - 0s 133us/sample - loss: 0.0372 - accuracy: 0.8533\n",
      "Epoch 242/300\n",
      "75/75 [==============================] - 0s 119us/sample - loss: 0.0387 - accuracy: 0.8533\n",
      "Epoch 243/300\n",
      "75/75 [==============================] - 0s 126us/sample - loss: 0.0423 - accuracy: 0.8533\n",
      "Epoch 244/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0343 - accuracy: 0.8533\n",
      "Epoch 245/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0335 - accuracy: 0.8533\n",
      "Epoch 246/300\n",
      "75/75 [==============================] - 0s 116us/sample - loss: 0.0388 - accuracy: 0.8533\n",
      "Epoch 247/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0361 - accuracy: 0.8533\n",
      "Epoch 248/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0363 - accuracy: 0.8533\n",
      "Epoch 249/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0414 - accuracy: 0.8533\n",
      "Epoch 250/300\n",
      "75/75 [==============================] - 0s 112us/sample - loss: 0.0384 - accuracy: 0.8533\n",
      "Epoch 251/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 252/300\n",
      "75/75 [==============================] - 0s 128us/sample - loss: 0.0373 - accuracy: 0.8533\n",
      "Epoch 253/300\n",
      "75/75 [==============================] - 0s 126us/sample - loss: 0.0380 - accuracy: 0.8533\n",
      "Epoch 254/300\n",
      "75/75 [==============================] - 0s 160us/sample - loss: 0.0333 - accuracy: 0.8533\n",
      "Epoch 255/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0415 - accuracy: 0.8533\n",
      "Epoch 256/300\n",
      "75/75 [==============================] - 0s 115us/sample - loss: 0.0345 - accuracy: 0.8533\n",
      "Epoch 257/300\n",
      "75/75 [==============================] - 0s 163us/sample - loss: 0.0363 - accuracy: 0.8533\n",
      "Epoch 258/300\n",
      "75/75 [==============================] - 0s 167us/sample - loss: 0.0381 - accuracy: 0.8533\n",
      "Epoch 259/300\n",
      "75/75 [==============================] - 0s 130us/sample - loss: 0.0424 - accuracy: 0.8533\n",
      "Epoch 260/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0348 - accuracy: 0.8533\n",
      "Epoch 261/300\n",
      "75/75 [==============================] - 0s 136us/sample - loss: 0.0409 - accuracy: 0.8533\n",
      "Epoch 262/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0384 - accuracy: 0.8533\n",
      "Epoch 263/300\n",
      "75/75 [==============================] - 0s 118us/sample - loss: 0.0390 - accuracy: 0.8533\n",
      "Epoch 264/300\n",
      "75/75 [==============================] - 0s 134us/sample - loss: 0.0371 - accuracy: 0.8533\n",
      "Epoch 265/300\n",
      "75/75 [==============================] - 0s 122us/sample - loss: 0.0421 - accuracy: 0.8533\n",
      "Epoch 266/300\n",
      "75/75 [==============================] - 0s 110us/sample - loss: 0.0341 - accuracy: 0.8533\n",
      "Epoch 267/300\n",
      "75/75 [==============================] - 0s 127us/sample - loss: 0.0365 - accuracy: 0.8533\n",
      "Epoch 268/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0344 - accuracy: 0.8533\n",
      "Epoch 269/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0366 - accuracy: 0.8533\n",
      "Epoch 270/300\n",
      "75/75 [==============================] - 0s 139us/sample - loss: 0.0363 - accuracy: 0.8533\n",
      "Epoch 271/300\n",
      "75/75 [==============================] - 0s 112us/sample - loss: 0.0390 - accuracy: 0.8533\n",
      "Epoch 272/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0393 - accuracy: 0.8533\n",
      "Epoch 273/300\n",
      "75/75 [==============================] - 0s 135us/sample - loss: 0.0400 - accuracy: 0.8533\n",
      "Epoch 274/300\n",
      "75/75 [==============================] - 0s 113us/sample - loss: 0.0374 - accuracy: 0.8533\n",
      "Epoch 275/300\n",
      "75/75 [==============================] - 0s 139us/sample - loss: 0.0361 - accuracy: 0.8533\n",
      "Epoch 276/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0385 - accuracy: 0.8533\n",
      "Epoch 277/300\n",
      "75/75 [==============================] - 0s 111us/sample - loss: 0.0399 - accuracy: 0.8533\n",
      "Epoch 278/300\n",
      "75/75 [==============================] - 0s 133us/sample - loss: 0.0377 - accuracy: 0.8533\n",
      "Epoch 279/300\n",
      "75/75 [==============================] - 0s 117us/sample - loss: 0.0391 - accuracy: 0.8533\n",
      "Epoch 280/300\n",
      "75/75 [==============================] - 0s 115us/sample - loss: 0.0341 - accuracy: 0.8533\n",
      "Epoch 281/300\n",
      "75/75 [==============================] - 0s 123us/sample - loss: 0.0385 - accuracy: 0.8533\n",
      "Epoch 282/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0408 - accuracy: 0.8533\n",
      "Epoch 283/300\n",
      "75/75 [==============================] - 0s 124us/sample - loss: 0.0401 - accuracy: 0.8533\n",
      "Epoch 284/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.86 - 0s 129us/sample - loss: 0.0398 - accuracy: 0.8533\n",
      "Epoch 285/300\n",
      "75/75 [==============================] - 0s 109us/sample - loss: 0.0399 - accuracy: 0.8533\n",
      "Epoch 286/300\n",
      "75/75 [==============================] - 0s 139us/sample - loss: 0.0352 - accuracy: 0.8533\n",
      "Epoch 287/300\n",
      "75/75 [==============================] - 0s 119us/sample - loss: 0.0388 - accuracy: 0.8533\n",
      "Epoch 288/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0363 - accuracy: 0.8533\n",
      "Epoch 289/300\n",
      "75/75 [==============================] - 0s 125us/sample - loss: 0.0345 - accuracy: 0.8533\n",
      "Epoch 290/300\n",
      "75/75 [==============================] - 0s 114us/sample - loss: 0.0410 - accuracy: 0.8533\n",
      "Epoch 291/300\n",
      "75/75 [==============================] - 0s 128us/sample - loss: 0.0387 - accuracy: 0.8533\n",
      "Epoch 292/300\n",
      "75/75 [==============================] - 0s 121us/sample - loss: 0.0384 - accuracy: 0.8533\n",
      "Epoch 293/300\n",
      "75/75 [==============================] - 0s 133us/sample - loss: 0.0380 - accuracy: 0.8533\n",
      "Epoch 294/300\n",
      "75/75 [==============================] - 0s 143us/sample - loss: 0.0370 - accuracy: 0.8533\n",
      "Epoch 295/300\n",
      "75/75 [==============================] - 0s 141us/sample - loss: 0.0326 - accuracy: 0.8533\n",
      "Epoch 296/300\n",
      "75/75 [==============================] - 0s 110us/sample - loss: 0.0371 - accuracy: 0.8533\n",
      "Epoch 297/300\n",
      "75/75 [==============================] - 0s 108us/sample - loss: 0.0361 - accuracy: 0.8533\n",
      "Epoch 298/300\n",
      "75/75 [==============================] - 0s 132us/sample - loss: 0.0389 - accuracy: 0.8533\n",
      "Epoch 299/300\n",
      "75/75 [==============================] - 0s 107us/sample - loss: 0.0361 - accuracy: 0.8533\n",
      "Epoch 300/300\n",
      "75/75 [==============================] - 0s 139us/sample - loss: 0.0372 - accuracy: 0.8533\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "epochs = 300\n",
    "batch_size = 15\n",
    "\n",
    "# class_weight = {0 : 1., 1: 1.,}  # The weights can be changed and made inversely proportional to the class size to improve the accuracy.\n",
    "class_weight = {0 : 0.12, 1: 0.88,}\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weight)\n",
    "    end_time = time.time()\n",
    "    time_k[k] = end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 0.5851851851851851\n",
      "Average Run time 3.7768200238545737\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an LSTM Classifier on the sequences for comparison\n",
    "We built an LSTM Classifier on the sequences to compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = darpa_data['seq']\n",
    "encoded_X = np.ndarray(shape=(len(X),), dtype=list)\n",
    "for i in range(0,len(X)):\n",
    "    encoded_X[i]=X.iloc[i].split(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = np.max(darpa_data['seqlen'])\n",
    "encoded_X = tf.keras.preprocessing.sequence.pad_sequences(encoded_X, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 6s 83ms/sample - loss: 0.6854 - accuracy: 0.7123\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 4s 53ms/sample - loss: 0.6532 - accuracy: 0.8904\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 4s 60ms/sample - loss: 0.6183 - accuracy: 0.8904\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 4s 50ms/sample - loss: 0.5585 - accuracy: 0.8904\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 4s 50ms/sample - loss: 0.4765 - accuracy: 0.8904\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 4s 50ms/sample - loss: 0.3678 - accuracy: 0.8904\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 3s 46ms/sample - loss: 0.3535 - accuracy: 0.8904\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 4s 49ms/sample - loss: 0.3570 - accuracy: 0.8904\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 3s 48ms/sample - loss: 0.3504 - accuracy: 0.8904\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 4s 52ms/sample - loss: 0.3497 - accuracy: 0.8904\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 4s 52ms/sample - loss: 0.3442 - accuracy: 0.8904\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3445 - accuracy: 0.8904\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 4s 50ms/sample - loss: 0.3442 - accuracy: 0.8904\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3455 - accuracy: 0.8904\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 4s 58ms/sample - loss: 0.3456 - accuracy: 0.8904\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 5s 75ms/sample - loss: 0.3424 - accuracy: 0.8904\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 4s 61ms/sample - loss: 0.3415 - accuracy: 0.8904\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 4s 49ms/sample - loss: 0.3412 - accuracy: 0.8904\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 4s 51ms/sample - loss: 0.3411 - accuracy: 0.8904\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 4s 48ms/sample - loss: 0.3388 - accuracy: 0.8904\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3365 - accuracy: 0.8904\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3351 - accuracy: 0.8904\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 3s 46ms/sample - loss: 0.3323 - accuracy: 0.8904\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3280 - accuracy: 0.8904\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3218 - accuracy: 0.8904\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3191 - accuracy: 0.8904\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.3051 - accuracy: 0.8904\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 4s 48ms/sample - loss: 0.2973 - accuracy: 0.8904\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 4s 55ms/sample - loss: 0.2814 - accuracy: 0.8904\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 4s 59ms/sample - loss: 0.2565 - accuracy: 0.8904\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 4s 49ms/sample - loss: 0.2399 - accuracy: 0.8904\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.2287 - accuracy: 0.8904\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.2114 - accuracy: 0.8904\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 3s 48ms/sample - loss: 0.1970 - accuracy: 0.8904\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.1867 - accuracy: 0.9315\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.2016 - accuracy: 0.9315\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 4s 48ms/sample - loss: 0.1786 - accuracy: 0.9315\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 3s 46ms/sample - loss: 0.1645 - accuracy: 0.9315\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.1976 - accuracy: 0.9315\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 3s 47ms/sample - loss: 0.2371 - accuracy: 0.9315\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 4s 51ms/sample - loss: 0.2020 - accuracy: 0.9452\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 4s 50ms/sample - loss: 0.1613 - accuracy: 0.9315\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 4s 49ms/sample - loss: 0.1621 - accuracy: 0.9315\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 4s 48ms/sample - loss: 0.1598 - accuracy: 0.9315\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 4s 51ms/sample - loss: 0.1606 - accuracy: 0.9315\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 4s 51ms/sample - loss: 0.1519 - accuracy: 0.9315\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 4s 49ms/sample - loss: 0.1510 - accuracy: 0.9315\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 4s 53ms/sample - loss: 0.1408 - accuracy: 0.9315\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 4s 50ms/sample - loss: 0.1420 - accuracy: 0.9452\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 4s 48ms/sample - loss: 0.1516 - accuracy: 0.9452\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 6s 78ms/sample - loss: 0.6809 - accuracy: 0.7838\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 4s 55ms/sample - loss: 0.6474 - accuracy: 0.8784\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.6016 - accuracy: 0.8784\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.5150 - accuracy: 0.8784\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.4244 - accuracy: 0.8784\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3681 - accuracy: 0.8784\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.3746 - accuracy: 0.8784\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3760 - accuracy: 0.8784\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3743 - accuracy: 0.8784\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3748 - accuracy: 0.8784\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.3707 - accuracy: 0.8784\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.3699 - accuracy: 0.8784\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3705 - accuracy: 0.8784\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3693 - accuracy: 0.8784\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3696 - accuracy: 0.8784\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3686 - accuracy: 0.8784\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.3694 - accuracy: 0.8784\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3677 - accuracy: 0.8784\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.3671 - accuracy: 0.8784\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3656 - accuracy: 0.8784\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3654 - accuracy: 0.8784\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.3638 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 4s 60ms/sample - loss: 0.3620 - accuracy: 0.8784\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 4s 55ms/sample - loss: 0.3603 - accuracy: 0.8784\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.3563 - accuracy: 0.8784\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.3519 - accuracy: 0.8784\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3464 - accuracy: 0.8784\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3326 - accuracy: 0.8784\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.3142 - accuracy: 0.8784\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.2920 - accuracy: 0.8784\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.2734 - accuracy: 0.8784\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.2719 - accuracy: 0.8784\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 5s 61ms/sample - loss: 0.3239 - accuracy: 0.8784\n",
      "Epoch 34/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.2953 - accuracy: 0.8784\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.2352 - accuracy: 0.8784\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.2597 - accuracy: 0.8784\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2323 - accuracy: 0.8784\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.2316 - accuracy: 0.8784\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2129 - accuracy: 0.8784\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.2135 - accuracy: 0.8919\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 4s 47ms/sample - loss: 0.1961 - accuracy: 0.8919\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.2080 - accuracy: 0.8919\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.2059 - accuracy: 0.9459\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 4s 58ms/sample - loss: 0.1858 - accuracy: 0.9459\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.1876 - accuracy: 0.9459\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1708 - accuracy: 0.9459\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1722 - accuracy: 0.9459\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.1699 - accuracy: 0.9595\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.1665 - accuracy: 0.9324\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1549 - accuracy: 0.9595\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 75 samples\n",
      "Epoch 1/50\n",
      "75/75 [==============================] - 5s 70ms/sample - loss: 0.6723 - accuracy: 0.8800\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.6358 - accuracy: 0.8800\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.5826 - accuracy: 0.8800\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.4889 - accuracy: 0.8800\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 3s 43ms/sample - loss: 0.3860 - accuracy: 0.8800\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 3s 41ms/sample - loss: 0.3645 - accuracy: 0.8800\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3717 - accuracy: 0.8800\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3727 - accuracy: 0.8800\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 3s 41ms/sample - loss: 0.3688 - accuracy: 0.8800\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 3s 41ms/sample - loss: 0.3681 - accuracy: 0.8800\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 3s 43ms/sample - loss: 0.3688 - accuracy: 0.8800\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 4s 48ms/sample - loss: 0.3708 - accuracy: 0.8800\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 3s 45ms/sample - loss: 0.3681 - accuracy: 0.8800\n",
      "Epoch 14/50\n",
      "75/75 [==============================] - 3s 46ms/sample - loss: 0.3671 - accuracy: 0.8800\n",
      "Epoch 15/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3668 - accuracy: 0.8800\n",
      "Epoch 16/50\n",
      "75/75 [==============================] - 4s 50ms/sample - loss: 0.3668 - accuracy: 0.8800\n",
      "Epoch 17/50\n",
      "75/75 [==============================] - 4s 47ms/sample - loss: 0.3671 - accuracy: 0.8800\n",
      "Epoch 18/50\n",
      "75/75 [==============================] - 4s 49ms/sample - loss: 0.3670 - accuracy: 0.8800\n",
      "Epoch 19/50\n",
      "75/75 [==============================] - 3s 45ms/sample - loss: 0.3665 - accuracy: 0.8800\n",
      "Epoch 20/50\n",
      "75/75 [==============================] - 4s 48ms/sample - loss: 0.3664 - accuracy: 0.8800\n",
      "Epoch 21/50\n",
      "75/75 [==============================] - 3s 46ms/sample - loss: 0.3679 - accuracy: 0.8800\n",
      "Epoch 22/50\n",
      "75/75 [==============================] - 4s 48ms/sample - loss: 0.3663 - accuracy: 0.8800\n",
      "Epoch 23/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.3658 - accuracy: 0.8800\n",
      "Epoch 24/50\n",
      "75/75 [==============================] - 4s 48ms/sample - loss: 0.3656 - accuracy: 0.8800\n",
      "Epoch 25/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3654 - accuracy: 0.8800\n",
      "Epoch 26/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3654 - accuracy: 0.8800\n",
      "Epoch 27/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.3643 - accuracy: 0.8800\n",
      "Epoch 28/50\n",
      "75/75 [==============================] - 3s 45ms/sample - loss: 0.3667 - accuracy: 0.8800\n",
      "Epoch 29/50\n",
      "75/75 [==============================] - 3s 43ms/sample - loss: 0.3631 - accuracy: 0.8800\n",
      "Epoch 30/50\n",
      "75/75 [==============================] - 3s 43ms/sample - loss: 0.3627 - accuracy: 0.8800\n",
      "Epoch 31/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.3610 - accuracy: 0.8800\n",
      "Epoch 32/50\n",
      "75/75 [==============================] - 3s 43ms/sample - loss: 0.3590 - accuracy: 0.8800\n",
      "Epoch 33/50\n",
      "75/75 [==============================] - 3s 43ms/sample - loss: 0.3546 - accuracy: 0.8800\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3496 - accuracy: 0.8800\n",
      "Epoch 35/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3425 - accuracy: 0.8800\n",
      "Epoch 36/50\n",
      "75/75 [==============================] - 3s 42ms/sample - loss: 0.3194 - accuracy: 0.8800\n",
      "Epoch 37/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.3053 - accuracy: 0.8800\n",
      "Epoch 38/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.2906 - accuracy: 0.8800\n",
      "Epoch 39/50\n",
      "75/75 [==============================] - 4s 51ms/sample - loss: 0.2786 - accuracy: 0.8800\n",
      "Epoch 40/50\n",
      "75/75 [==============================] - 3s 46ms/sample - loss: 0.3188 - accuracy: 0.8800\n",
      "Epoch 41/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.2884 - accuracy: 0.8800\n",
      "Epoch 42/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.3086 - accuracy: 0.8800\n",
      "Epoch 43/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.2544 - accuracy: 0.8800\n",
      "Epoch 44/50\n",
      "75/75 [==============================] - 3s 44ms/sample - loss: 0.2654 - accuracy: 0.8800\n",
      "Epoch 45/50\n",
      "75/75 [==============================] - 5s 67ms/sample - loss: 0.2577 - accuracy: 0.8800\n",
      "Epoch 46/50\n",
      "75/75 [==============================] - 4s 56ms/sample - loss: 0.2398 - accuracy: 0.8800\n",
      "Epoch 47/50\n",
      "75/75 [==============================] - 4s 49ms/sample - loss: 0.2332 - accuracy: 0.8800\n",
      "Epoch 48/50\n",
      "75/75 [==============================] - 4s 48ms/sample - loss: 0.2239 - accuracy: 0.8933\n",
      "Epoch 49/50\n",
      "75/75 [==============================] - 4s 48ms/sample - loss: 0.2273 - accuracy: 0.8933\n",
      "Epoch 50/50\n",
      "75/75 [==============================] - 3s 46ms/sample - loss: 0.2289 - accuracy: 0.8933\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 15\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "\n",
    "for train_index, test_index in skf.split(encoded_X, y):\n",
    "    X_train, X_test = encoded_X[train_index], encoded_X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    embedding_vecor_length = 32\n",
    "    top_words=50\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    end_time=time.time()\n",
    "    time_k[k]=end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred=model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k]=sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 0.48888888888888893\n",
      "Average Run time 180.2923804918925\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the LSTM classifier gives an F1 score of 0. This may be improved by changing the model. However, we find that the SGT embedding could work with a small and unbalanced data without the need of a complicated classifier model.\n",
    "\n",
    "LSTM models typically require more data for training and also has significantly more computation time. The LSTM model above took 425.6 secs while the MLP model took just 9.1 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
